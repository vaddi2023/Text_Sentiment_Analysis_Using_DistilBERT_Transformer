{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Sentiment Analysis using DistilBERT Transformer\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Check device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load and sample IMDB dataset\n",
    "dataset = load_dataset(\"imdb\")\n",
    "small_train_dataset = dataset['train'].shuffle(seed=42).select(range(2000))\n",
    "small_test_dataset = dataset['test'].shuffle(seed=42).select(range(1000))\n",
    "\n",
    "# Display label distribution\n",
    "labels = [example['label'] for example in small_train_dataset]\n",
    "label_df = pd.DataFrame(labels, columns=['label'])\n",
    "label_df['label'] = label_df['label'].map({0: 'Negative', 1: 'Positive'})\n",
    "sns.countplot(data=label_df, x='label')\n",
    "plt.title(\"Label Distribution in Sampled IMDB Training Set\")\n",
    "plt.show()\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
    "model.to(device)\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding='max_length', truncation=True, max_length=512)\n",
    "\n",
    "# Tokenize datasets\n",
    "tokenized_dataset = small_train_dataset.map(tokenize, batched=True)\n",
    "tokenized_test = small_test_dataset.map(tokenize, batched=True)\n",
    "tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "tokenized_test.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "# Split train/validation\n",
    "train_size = int(0.8 * len(tokenized_dataset))\n",
    "val_size = len(tokenized_dataset) - train_size\n",
    "tokenized_train, tokenized_val = random_split(tokenized_dataset, [train_size, val_size])\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(tokenized_train, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(tokenized_val, batch_size=16)\n",
    "test_loader = DataLoader(tokenized_test, batch_size=16)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Training loop with validation\n",
    "EPOCHS = 3\n",
    "results = []\n",
    "model.train()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    for batch in loop:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        loop.set_description(f\"Epoch {epoch+1}\")\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_preds, val_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            preds = torch.argmax(outputs.logits, axis=1)\n",
    "            val_preds.extend(preds.cpu().numpy())\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    report = classification_report(val_labels, val_preds, target_names=['Negative', 'Positive'], output_dict=True)\n",
    "    accuracy = report['accuracy']\n",
    "    results.append({\"epoch\": epoch+1, \"loss\": total_loss / len(train_loader), \"val_accuracy\": accuracy})\n",
    "    model.train()\n",
    "\n",
    "# Save model\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "torch.save(model.state_dict(), \"models/distilbert_imdb.pt\")\n",
    "\n",
    "# Save tokenizer\n",
    "tokenizer.save_pretrained(\"models/tokenizer/\")\n",
    "\n",
    "\n",
    "# Evaluation on test set\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        preds = torch.argmax(outputs.logits, axis=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Classification metrics\n",
    "print(classification_report(all_labels, all_preds, target_names=['Negative', 'Positive']))\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "test_accuracy = accuracy_score(all_labels, all_preds)\n",
    "print(f\"\\nTest Set Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Plot training loss\n",
    "plt.figure()\n",
    "plt.plot(results_df[\"epoch\"], results_df[\"loss\"], marker='o', label='Training Loss')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss per Epoch\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot validation accuracy\n",
    "plt.figure()\n",
    "plt.plot(results_df[\"epoch\"], results_df[\"val_accuracy\"], marker='o', color='green', label='Validation Accuracy')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Validation Accuracy per Epoch\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Predict custom review sentiment\n",
    "def predict_sentiment(text):\n",
    "    model.eval()\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        pred = torch.argmax(outputs.logits, axis=1).item()\n",
    "    return \"Positive\" if pred == 1 else \"Negative\"\n",
    "\n",
    "# Test prediction\n",
    "example_review = \"This movie was absolutely fantastic, with great performances!\"\n",
    "print(f\"\\nTest Review Sentiment: {predict_sentiment(example_review)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
